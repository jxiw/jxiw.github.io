<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Junxiong Wang</title>
  <meta name="author" content="Junxiong Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <script async data-id="101472372" src="//static.getclicky.com/js"></script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-348H08GT4K"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-348H08GT4K');
  </script>
</head>

<body>
  <table style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <!-- <td style="padding:2.5%;width:120%;vertical-align:middle"> -->
                <td style="width:100%;max-width:820px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <!-- <td style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"> -->
                  <p style="text-align:center">
                    <span>Junxiong Wang</span>
                  </p>
                  <p>

                    I obtained my PhD in Computer Science from Cornell University, where I worked at the intersection of systems and large language models, with a focus on linear models and their hybrid variants.
                    
                    I lead multiple research projects at Together AI, 
                    including <a href="https://www.together.ai/blog/adaptive-learning-speculator-system-atlas">adaptive speculative decoding</a>, inference time training, and efficient RL rollouts.
                    
                    <strong>If you would like to see my CV, please feel free to contact me by email.</strong>
                    
                    <!-- https://www.together.ai/blog/adaptive-learning-speculator-system-atlas

                    https://www.together.ai/blog/fastest-inference-for-deepseek-r1-0528-with-nvidia-hgx-b200

                    https://venturebeat.com/ai/together-ais-atlas-adaptive-speculator-delivers-400-inference-speedup-by -->
                    
                  <!-- My research focuses on:</p>
                  
                  <p>ML and system approaches to modeling long sequences:</p>
                  
                  <ul>
                    <li>We introduce the first bidirectional linear-complexity language model <a href="https://arxiv.org/abs/2212.10544">BiGS</a> that matches <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> performance without using attention.</li>
                    <li>We demonstrate that linear RNNs also outperform transformers in <a href="https://openreview.net/forum?id=X1xNsuKssb">byte-level language modeling</a> (high resolution and noise data), enabling the universal representation of different modalities and formats.</li>
                    <li>Training LLMs from scratch is costly, we explore distilling large transformers into linear RNNs. Our distillation approach, <a href="https://arxiv.org/abs/2408.15237">MambaInLlama</a> utilizes only academic budget resources and outperforms some models trained from scratch using industry scale GPUs.
                  </ul> -->
                  
<!--                   I also had looked at
                  <p>ML for data system:</p>
                  <ul>
                    <li>
                    Modern data systems face challenges in performance optimization due to unreliable assumptions about data distribution and intermediate results. 
                    We propose replacing traditional cost models with a sequential decision-making approach using Monte Carlo Tree Search (MCTS) and leveraging runtime feedback as a reward. 
                    We introduce and apply this paradigm to <a href="https://dl.acm.org/doi/10.1145/3299869.3300088">query optimization</a>, 
                    <a href="https://www.microsoft.com/en-us/research/publication/budget-aware-index-tuning-with-reinforcement-learning-extended-version/">index recommendations</a>, 
                    <a href="https://dl.acm.org/doi/10.14778/3611479.3611489">graph query optimization</a>,
                    and <a href="https://dl.acm.org/doi/10.14778/3484224.3484236">general universal optimization</a>.
                  </li>
                  </ul> -->

                  <!-- I was incredibly fortunate to have spent my summers working with outstanding researchers at <a href="https://machinelearning.apple.com/research/entity-disambiguation-fusion-decoding">Apple AI/ML Siri & Information Intelligence (2023)</a>, Microsoft Research (2020). -->

                  <!-- <p>
                    Open-source packages:<a href="https://github.com/jxiw/varlen_mamba">Variable-length Hybrid Mamba CUDA kernel</a>,
                    <a href="https://github.com/jxiw/M1/tree/main/sft">Mamba distillation based on Axolot</a>, <a href="https://github.com/jxiw/MambaInLlama">based on TRL</a>,
                    and <a href="https://github.com/jxiw/M1/tree/main/rl">RL training based on VeRL</a>.
                  </p> -->

                  
                  <!-- <!-- <div style="font-size: 0.9em;"> -->
                    <h3>Recent Publications</h3>
                    <ul>

                      <li>
                        Haojun Xia*, Xiaoxia Wu*, Jisen Li*, Robert Wu, <strong>Junxiong Wang</strong>, Jue Wang, Chenxi Li, Aman Singhal, Alay Dilipbhai Shah, Alpay Ariyak, Donglin Zhuang, Zhongzhu Zhou, Ben Athiwaratkun, Zhen Zheng, Shuaiwen Leon Song<br>
                        <a href="https://arxiv.org/pdf/2511.18643">Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost</a><br>
                        In submission, 2025<br>
                        <br>
                      </li>
                      
                      <li>
                        Zelei Shao*, Vikranth Srivatsa*, Sanjana Srivastava, Qingyang Wu, Alpay Ariyak, Xiaoxia Wu, Ameen Patel, Jue Wang, Percy Liang, Tri Dao, Ce Zhang, Yiying Zhang, Ben Athiwaratkun, Chenfeng Xu, <strong>Junxiong Wang</strong><br>
                        <a href="https://arxiv.org/pdf/2511.13841">Beat the long tail: Distribution-Aware Speculative Decoding for RL Training</a><br>
                        In submission, 2025<br>
                        <br>
                      </li>
                      
                      <li>
                        Costin-Andrei Oncescu, Qingyang Wu, Wai Tong Chung, Robert Wu, Bryan Gopal, <strong>Junxiong Wang</strong>, Tri Dao, Ben Athiwaratkun<br>
                        <a href="https://arxiv.org/pdf/2511.02237">Opportunistic Expert Activation: Batch-Aware Expert Routing for Faster Decode Without Retraining</a><br>
                        In submission, 2025<br>
                        <br>
                      </li>
                      
                      <li>
                        Jiaqi Leng*, Xiang Hu*, <strong>Junxiong Wang</strong>, Jianguo Li, Wei Wu, Yucheng Lu<br>
                        <a href="https://arxiv.org/pdf/2510.17196">Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models</a><br>
                        In submission, 2025<br>
                        <br>
                      </li>

                      <li>
                        Woojeong Kim, <strong>Junxiong Wang</strong>, Jing Nathan Yan, Mohamed S. Abdelfattah, Alexander M. Rush<br>
                        <a href="https://www.arxiv.org/pdf/2508.08446">Overfill: Two-Stage Models for Efficient Language Model Decoding</a><br>
                         Conference on Language Modeling (CoLM), 2025<br>
                        <br>
                      </li>

                      <!-- <li>
                        Daniele Paliotta*, <strong>Junxiong Wang*</strong>, Matteo Pagliardini*, Kevin Y Li*, Aviv Bick, J Zico Kolter, Albert Gu, Fran√ßois Fleuret, Tri Dao <br>
                        <a href="https://arxiv.org/pdf/2502.20339">Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners
                        </a><br>
                        A shorter version at ICLR 2024, Workshop on Reasoning and Planning for Large Language Models<br>
                        In submission<br>
                        <br>
                      </li> -->

                     <li>
                        <strong>Junxiong Wang</strong>, Wen-Ding Li, Daniele Paliotta, Daniel Ritter, Alexander M. Rush, Tri Dao <br>
                        <a href="https://arxiv.org/abs/2504.10449">M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models
                        </a><br>
                        Workshop on Efficient Reasoning (Best Paper Award), Neural Information Processing Systems (NeurIPS), 2025<br>
                        <br>
                      </li>


                      <li>
                        <strong>Junxiong Wang*</strong>, Daniele Paliotta*, Avner May, Alexander M. Rush, Tri Dao <br>
                        <a href="https://arxiv.org/pdf/2408.15237v3">The Mamba in the Llama: Distilling and Accelerating Hybrid Models</a><br>
                        <a href="https://huggingface.co/JunxiongWang">Models</a>, <a href="https://www.youtube.com/watch?v=A5ff8hu1amM">Video</a>, <a href="https://github.com/jxiw/MambaInLlama">Code</a>, <a href="https://www.together.ai/blog/the-mamba-in-the-llama-distilling-and-accelerating-hybrid-models">Blog</a> <br>
                        Neural Information Processing Systems (NeurIPS), 2024 <br>
                        A shorter version at ICML 2024, 2nd Workshop on Efficient Systems for Foundation Models (ES-FoMo) <br>
                        <br>
                      </li>
          
                      <!-- <li>
                        <strong>Junxiong Wang</strong>, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush <br>
                        <a href="https://arxiv.org/pdf/2401.13660.pdf">MambaByte: Token-free Selective State Space Model</a> <br>
                        <a href="https://huggingface.co/collections/JunxiongWang/mambabyte-66de59f9ecc44bd637946442">Models</a>, <a href="https://www.youtube.com/watch?v=kcd0BTKJuXk">Video</a> <br>
                        Conference on Language Modeling (CoLM), 2024<br>
                        <br>
                      </li> -->
  
                      <!-- <li>
                        <strong>Junxiong Wang</strong>, Ali Mousavi, Omar Attia, Saloni Potdar, Alexander M. Rush, Umar Farooq Minhas, Yunyao Li <br>
                        <a href="https://machinelearning.apple.com/research/entity-disambiguation-fusion-decoding">Entity Disambiguation via Fusion Entity Decoding</a><br>
                        North American Chapter of the Association for Computational Linguistics (NAACL), 2024 <br>
                        <br>
                      </li>  -->
  
<!--                       <li>
                        <strong>Junxiong Wang*</strong>, Kaiwen Wang*, Yueying Li, Nathan Kallus, Immanuel Trummer, Wen Sun <br>
                        <a href="https://rlj.cs.umass.edu/2024/papers/Paper14.html">JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning</a><br>
                        Reinforcement Learning Conference (RLC), 2024, 
                        <a href="https://github.com/kaiwenw/JoinGym">Code</a><br>
                        <br>
                      </li> -->
                      
<!--                       <li>
                        <strong>Junxiong Wang</strong>, Jing Nathan Yan, Albert Gu, Alexander M. Rush <br>
                        <a href="https://arxiv.org/pdf/2212.10544.pdf">Pretraining Without Attention </a> <br>
                        Findings of Empirical Methods in Natural Language Processing (EMNLP), 2022 <br>
                        <a href="https://github.com/jxiw/BiGS">Code</a>,
                        <a href="https://huggingface.co/JunxiongWang?search_models=BiGS">Models</a>,
                        <a href="https://www.cs.cornell.edu/~junxiong/Pretrain_S4_Slides.pdf">Slides</a><br>
                        <br>
                      </li> -->

<!--                       <li>
                        <strong>Junxiong Wang</strong>, Mitchell Gray, Immanuel Trummer, Ahmet Kara, Dan Olteanu <br>
                        <a href="https://arxiv.org/pdf/2307.16540.pdf">ADOPT: Adaptively Optimizing Attribute Orders for Worst-Case Optimal Join Algorithms via Reinforcement
                          Learning </a>
                        <br>
                        International Conference on Very Large Data Bases (VLDB), 2023, <a href="https://github.com/jxiw/ADOPT">Code</a><br>
                        <br>
                      </li> -->

<!--                       <li>
                        <strong>Junxiong Wang</strong>, Debabrota Basu, Immanuel Trummer<br>
                        <a href="https://arxiv.org/pdf/2110.07232.pdf">Procrastinated Tree Search: Black-box Optimization with Delayed, Noisy, and Multi-fidelity Feedback</a><br>
                        AAAI Conference on Artificial Intelligence (AAAI), 2022, <a href="https://www.cs.cornell.edu/~junxiong/pcts_slide.pdf">Slides</a> <br>
                        <br>
                      </li> -->
  
<!--                       <li>
                        <strong>Junxiong Wang</strong>, Immanuel Trummer, Debabrota Basu <br>
                        <a href="https://arxiv.org/pdf/2104.01744.pdf">UDO: Universal Database Optimization using Reinforcement Learning</a><br>
                        International Conference on Very Large Data Bases (VLDB), 2022, <a href="https://github.com/jxiw/UDO">Code</a><br>
                        <br>
                      </li> -->
  
<!--                       <li>
                        Immanuel Trummer, <strong>Junxiong Wang</strong>, Deepak Maram, Saehan Jo, Samuel Moseley,
                        Joseph Antonakakis<br>
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3299869.3300088">
                        SkinnerDB: Regret-bounded Query Evaluation via Reinforcement Learning </a> <br>
                        ACM SIGMOD International Conference on Management of Data (SIGMOD), 2019 <br>
                        Best of SIGMOD, <a href="https://dl.acm.org/doi/fullHtml/10.1145/3464389">extended version</a> in ACM Transactions on Database Systems (TODS), 2021 <br>
                      </li>   -->
                     

                    </ul>
                  </div>

                  <p style="text-align:center">
                    <!-- <a href="mailto:junxiong@cs.cornell.edu">Email</a> &nbsp;/&nbsp; -->
                    Email:Firstname@cs.cornell.edu &nbsp;/&nbsp;
                    <a href="https://github.com/jxiw">Github</a> &nbsp;/&nbsp;
                    <a href="https://huggingface.co/JunxiongWang">HuggingFace Models</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?hl=en&user=PaQDVTwAAAAJ&view_op=list_works&sortby=pubdate">Papers </a>  &nbsp;/&nbsp;
                    <a href="https://x.com/_junxiong_wang">Twitter</a>
                  </p>
                </td>

                <td style="width:30%;vertical-align:top;text-align:center;">
                  <!-- Add margins to push the image slightly -->
                  <img style="margin-top:20px; margin-left:20px; width:210px;height:auto;" 
                       alt="Profile photo of Junxiong Wang" 
                       src="https://www.cs.cornell.edu/~junxiong/homepage/photo.jpg" 
                       class="hoverZoomLink">
                </td>

              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
  
</body>
</html>
